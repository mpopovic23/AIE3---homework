{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gJXW_DgiSebM"
      },
      "source": [
        "# LangGraph and LangSmith - Agentic RAG Powered by LangChain\n",
        "\n",
        "In the following notebook we'll complete the following tasks:\n",
        "\n",
        "- ü§ù Breakout Room #1:\n",
        "  1. Install required libraries\n",
        "  2. Set Environment Variables\n",
        "  3. Creating our Tool Belt\n",
        "  4. Creating Our State\n",
        "  5. Creating and Compiling A Graph!\n",
        "  \n",
        "- ü§ù Breakout Room #2:\n",
        "  - Part 1:\n",
        "    1. Creating an Evaluation Dataset\n",
        "    2. Adding Evaluators\n",
        "    3. Evaluating\n",
        "  - Part 2:\n",
        "    1. Adding conditional check."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "djQ3nRAgoF67"
      },
      "source": [
        "# ü§ù Breakout Room #1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e7pQDUhUnIo8"
      },
      "source": [
        "## Part 1: LangGraph - Building Cyclic Applications with LangChain\n",
        "\n",
        "LangGraph is a tool that leverages LangChain Expression Language to build coordinated multi-actor and stateful applications that includes cyclic behaviour.\n",
        "\n",
        "### Why Cycles?\n",
        "\n",
        "In essence, we can think of a cycle in our graph as a more robust and customizable loop. It allows us to keep our application agent-forward while still giving the powerful functionality of traditional loops.\n",
        "\n",
        "Due to the inclusion of cycles over loops, we can also compose rather complex flows through our graph in a much more readable and natural fashion. Effetively allowing us to recreate appliation flowcharts in code in an almost 1-to-1 fashion.\n",
        "\n",
        "### Why LangGraph?\n",
        "\n",
        "Beyond the agent-forward approach - we can easily compose and combine traditional \"DAG\" (directed acyclic graph) chains with powerful cyclic behaviour due to the tight integration with LCEL. This means it's a natural extension to LangChain's core offerings!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3_fLDElOVoop"
      },
      "source": [
        "## Task 1:  Dependencies\n",
        "\n",
        "We'll first install all our required libraries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KaVwN269EttM",
        "outputId": "3b97db0d-d119-4b43-b964-47291c7dba1c"
      },
      "outputs": [],
      "source": [
        "! pip install -qU langchain langchain_openai langchain-community langgraph arxiv duckduckgo_search==5.3.1b1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wujPjGJuoPwg"
      },
      "source": [
        "## Task 2: Environment Variables\n",
        "\n",
        "We'll want to set both our OpenAI API key and our LangSmith environment variables."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jdh8CoVWHRvs",
        "outputId": "761167a9-b570-421b-eb9c-8be3dc813f47"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import getpass\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"OpenAI API Key:\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nv0glIDyHmRt",
        "outputId": "b0237b19-ada7-4836-edc2-228e694a5ecb"
      },
      "outputs": [],
      "source": [
        "from uuid import uuid4\n",
        "\n",
        "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
        "os.environ[\"LANGCHAIN_PROJECT\"] = f\"AIE3 - LangGraph - {uuid4().hex[0:8]}\"\n",
        "os.environ[\"LANGCHAIN_API_KEY\"] = getpass.getpass(\"LangSmith API Key: \")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sBRyQmEAVzua"
      },
      "source": [
        "## Task 3: Creating our Tool Belt\n",
        "\n",
        "As is usually the case, we'll want to equip our agent with a toolbelt to help answer questions and add external knowledge.\n",
        "\n",
        "There's a tonne of tools in the [LangChain Community Repo](https://github.com/langchain-ai/langchain/tree/master/libs/community/langchain_community/tools) but we'll stick to a couple just so we can observe the cyclic nature of LangGraph in action!\n",
        "\n",
        "We'll leverage:\n",
        "\n",
        "- [Duck Duck Go Web Search](https://github.com/langchain-ai/langchain/tree/master/libs/community/langchain_community/tools/ddg_search)\n",
        "- [Arxiv](https://github.com/langchain-ai/langchain/tree/master/libs/community/langchain_community/tools/arxiv)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2k6n_Dob2F46"
      },
      "source": [
        "####üèóÔ∏è Activity #1:\n",
        "\n",
        "Please add the tools to use into our toolbelt.\n",
        "\n",
        "> NOTE: Each tool in our toolbelt should be a method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "lAxaSvlfIeOg"
      },
      "outputs": [],
      "source": [
        "from langchain_community.tools.ddg_search import DuckDuckGoSearchRun\n",
        "from langchain_community.tools.arxiv.tool import ArxivQueryRun\n",
        "\n",
        "tool_belt = [\n",
        "    DuckDuckGoSearchRun(),\n",
        "    ArxivQueryRun()\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1FdOjEslXdRR"
      },
      "source": [
        "### Actioning with Tools\n",
        "\n",
        "Now that we've created our tool belt - we need to create a process that will let us leverage them when we need them.\n",
        "\n",
        "We'll use the built-in [`ToolExecutor`](https://github.com/langchain-ai/langgraph/blob/fab950acfbf5fea46c9313dca34ee2ae01f1728b/libs/langgraph/langgraph/prebuilt/tool_executor.py#L50) to do so."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "cFr1m80-JZsD"
      },
      "outputs": [],
      "source": [
        "from langgraph.prebuilt import ToolExecutor\n",
        "\n",
        "tool_executor = ToolExecutor(tool_belt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VI-C669ZYVI5"
      },
      "source": [
        "### Model\n",
        "\n",
        "Now we can set-up our model! We'll leverage the familiar OpenAI model suite for this example - but it's not *necessary* to use with LangGraph. LangGraph supports all models - though you might not find success with smaller models - as such, they recommend you stick with:\n",
        "\n",
        "- OpenAI's GPT-3.5 and GPT-4\n",
        "- Anthropic's Claude\n",
        "- Google's Gemini\n",
        "\n",
        "> NOTE: Because we're leveraging the OpenAI function calling API - we'll need to use OpenAI *for this specific example* (or any other service that exposes an OpenAI-style function calling API."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "QkNS8rNZJs4z"
      },
      "outputs": [],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "model = ChatOpenAI(model=\"gpt-4o\", temperature=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ugkj3GzuZpQv"
      },
      "source": [
        "Now that we have our model set-up, let's \"put on the tool belt\", which is to say: We'll bind our LangChain formatted tools to the model in an OpenAI function calling format."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "4OdMqFafZ_0V"
      },
      "outputs": [],
      "source": [
        "from langchain_core.utils.function_calling import convert_to_openai_function\n",
        "\n",
        "functions = [convert_to_openai_function(t) for t in tool_belt]\n",
        "model = model.bind_functions(functions)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ERzuGo6W18Lr"
      },
      "source": [
        "#### ‚ùì Question #1:\n",
        "\n",
        "How does the model determine which tool to use?\n",
        "\n",
        "ANSWER: Based on the prompt, the descriptions of the tools, and the inputs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_296Ub96Z_H8"
      },
      "source": [
        "## Task 4: Putting the State in Stateful\n",
        "\n",
        "Earlier we used this phrasing:\n",
        "\n",
        "`coordinated multi-actor and stateful applications`\n",
        "\n",
        "So what does that \"stateful\" mean?\n",
        "\n",
        "To put it simply - we want to have some kind of object which we can pass around our application that holds information about what the current situation (state) is. Since our system will be constructed of many parts moving in a coordinated fashion - we want to be able to ensure we have some commonly understood idea of that state.\n",
        "\n",
        "LangGraph leverages a `StatefulGraph` which uses an `AgentState` object to pass information between the various nodes of the graph.\n",
        "\n",
        "There are more options than what we'll see below - but this `AgentState` object is one that is stored in a `TypedDict` with the key `messages` and the value is a `Sequence` of `BaseMessages` that will be appended to whenever the state changes.\n",
        "\n",
        "Let's think about a simple example to help understand exactly what this means (we'll simplify a great deal to try and clearly communicate what state is doing):\n",
        "\n",
        "1. We initialize our state object:\n",
        "  - `{\"messages\" : []}`\n",
        "2. Our user submits a query to our application.\n",
        "  - New State: `HumanMessage(#1)`\n",
        "  - `{\"messages\" : [HumanMessage(#1)}`\n",
        "3. We pass our state object to an Agent node which is able to read the current state. It will use the last `HumanMessage` as input. It gets some kind of output which it will add to the state.\n",
        "  - New State: `AgentMessage(#1, additional_kwargs {\"function_call\" : \"WebSearchTool\"})`\n",
        "  - `{\"messages\" : [HumanMessage(#1), AgentMessage(#1, ...)]}`\n",
        "4. We pass our state object to a \"conditional node\" (more on this later) which reads the last state to determine if we need to use a tool - which it can determine properly because of our provided object!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "mxL9b_NZKUdL"
      },
      "outputs": [],
      "source": [
        "from typing import TypedDict, Annotated\n",
        "from langgraph.graph.message import add_messages\n",
        "import operator\n",
        "from langchain_core.messages import BaseMessage\n",
        "\n",
        "class AgentState(TypedDict):\n",
        "  messages: Annotated[list, add_messages]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vWsMhfO9grLu"
      },
      "source": [
        "## Task 5: It's Graphing Time!\n",
        "\n",
        "Now that we have state, and we have tools, and we have an LLM - we can finally start making our graph!\n",
        "\n",
        "Let's take a second to refresh ourselves about what a graph is in this context.\n",
        "\n",
        "Graphs, also called networks in some circles, are a collection of connected objects.\n",
        "\n",
        "The objects in question are typically called nodes, or vertices, and the connections are called edges.\n",
        "\n",
        "Let's look at a simple graph.\n",
        "\n",
        "![image](https://i.imgur.com/2NFLnIc.png)\n",
        "\n",
        "Here, we're using the coloured circles to represent the nodes and the yellow lines to represent the edges. In this case, we're looking at a fully connected graph - where each node is connected by an edge to each other node.\n",
        "\n",
        "If we were to think about nodes in the context of LangGraph - we would think of a function, or an LCEL runnable.\n",
        "\n",
        "If we were to think about edges in the context of LangGraph - we might think of them as \"paths to take\" or \"where to pass our state object next\".\n",
        "\n",
        "Let's create some nodes and expand on our diagram.\n",
        "\n",
        "> NOTE: Due to the tight integration with LCEL - we can comfortably create our nodes in an async fashion!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "91flJWtZLUrl"
      },
      "outputs": [],
      "source": [
        "from langgraph.prebuilt import ToolInvocation\n",
        "import json\n",
        "from langchain_core.messages import FunctionMessage\n",
        "\n",
        "def call_model(state):\n",
        "  messages = state[\"messages\"]\n",
        "  response = model.invoke(messages)\n",
        "  return {\"messages\" : [response]}\n",
        "\n",
        "def call_tool(state):\n",
        "  last_message = state[\"messages\"][-1]\n",
        "\n",
        "  action = ToolInvocation(\n",
        "      tool=last_message.additional_kwargs[\"function_call\"][\"name\"],\n",
        "      tool_input=json.loads(\n",
        "          last_message.additional_kwargs[\"function_call\"][\"arguments\"]\n",
        "      )\n",
        "  )\n",
        "\n",
        "  response = tool_executor.invoke(action)\n",
        "\n",
        "  function_message = FunctionMessage(content=str(response), name=action.tool)\n",
        "\n",
        "  return {\"messages\" : [function_message]}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2bwR7MgWj3Wg"
      },
      "source": [
        "Now we have two total nodes. We have:\n",
        "\n",
        "- `call_model` is a node that will...well...call the model\n",
        "- `call_tool` is a node which will call a tool\n",
        "\n",
        "Let's start adding nodes! We'll update our diagram along the way to keep track of what this looks like!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "_vF4_lgtmQNo"
      },
      "outputs": [],
      "source": [
        "from langgraph.graph import StateGraph, END\n",
        "\n",
        "workflow = StateGraph(AgentState)\n",
        "\n",
        "workflow.add_node(\"agent\", call_model)\n",
        "workflow.add_node(\"action\", call_tool)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b8CjRlbVmRpW"
      },
      "source": [
        "Let's look at what we have so far:\n",
        "\n",
        "![image](https://i.imgur.com/md7inqG.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uaXHpPeSnOWC"
      },
      "source": [
        "Next, we'll add our entrypoint. All our entrypoint does is indicate which node is called first."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "YGCbaYqRnmiw"
      },
      "outputs": [],
      "source": [
        "workflow.set_entry_point(\"agent\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BUsfGoSpoF9U"
      },
      "source": [
        "![image](https://i.imgur.com/wNixpJe.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Q_pQgHmoW0M"
      },
      "source": [
        "Now we want to build a \"conditional edge\" which will use the output state of a node to determine which path to follow.\n",
        "\n",
        "We can help conceptualize this by thinking of our conditional edge as a conditional in a flowchart!\n",
        "\n",
        "Notice how our function simply checks if there is a \"function_call\" kwarg present.\n",
        "\n",
        "Then we create an edge where the origin node is our agent node and our destination node is *either* the action node or the END (finish the graph).\n",
        "\n",
        "It's important to highlight that the dictionary passed in as the third parameter (the mapping) should be created with the possible outputs of our conditional function in mind. In this case `should_continue` outputs either `\"end\"` or `\"continue\"` which are subsequently mapped to the action node or the END node."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "1BZgb81VQf9o"
      },
      "outputs": [],
      "source": [
        "def should_continue(state):\n",
        "  last_message = state[\"messages\"][-1]\n",
        "\n",
        "  if \"function_call\" not in last_message.additional_kwargs:\n",
        "    return \"end\"\n",
        "\n",
        "  return \"continue\"\n",
        "\n",
        "workflow.add_conditional_edges(\n",
        "    \"agent\",\n",
        "    should_continue,\n",
        "    {\n",
        "        \"continue\" : \"action\",\n",
        "        \"end\" : END\n",
        "    }\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Cvhcf4jp0Ce"
      },
      "source": [
        "Let's visualize what this looks like.\n",
        "\n",
        "![image](https://i.imgur.com/8ZNwKI5.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yKCjWJCkrJb9"
      },
      "source": [
        "Finally, we can add our last edge which will connect our action node to our agent node. This is because we *always* want our action node (which is used to call our tools) to return its output to our agent!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "UvcgbHf1rIXZ"
      },
      "outputs": [],
      "source": [
        "workflow.add_edge(\"action\", \"agent\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EiWDwBQtrw7Z"
      },
      "source": [
        "Let's look at the final visualization.\n",
        "\n",
        "![image](https://i.imgur.com/NWO7usO.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KYqDpErlsCsu"
      },
      "source": [
        "All that's left to do now is to compile our workflow - and we're off!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "zt9-KS8DpzNx"
      },
      "outputs": [],
      "source": [
        "app = workflow.compile()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xhNWIwBL1W4Q"
      },
      "source": [
        "#### ‚ùì Question #2:\n",
        "\n",
        "Is there any specific limit to how many times we can cycle?\n",
        "\n",
        "If not, how could we impose a limit to the number of cycles?\n",
        "\n",
        "ANSWER: We could add some cycle counter and when limit is reached move to termination"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GSCds6zTL5VJ"
      },
      "source": [
        "#### Helper Function to print messages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "xRPF0X5iL8Bh"
      },
      "outputs": [],
      "source": [
        "def print_messages(messages):\n",
        "  next_is_tool = False\n",
        "  initial_query = True\n",
        "  for message in messages[\"messages\"]:\n",
        "    if \"function_call\" in message.additional_kwargs:\n",
        "      print()\n",
        "      print(f'Tool Call - Name: {message.additional_kwargs[\"function_call\"][\"name\"]} + Query: {message.additional_kwargs[\"function_call\"][\"arguments\"]}')\n",
        "      next_is_tool = True\n",
        "      continue\n",
        "    if next_is_tool:\n",
        "      print(f\"Tool Response: {message.content}\")\n",
        "      next_is_tool = False\n",
        "      continue\n",
        "    if initial_query:\n",
        "      print(f\"Initial Query: {message.content}\")\n",
        "      print()\n",
        "      initial_query = False\n",
        "      continue\n",
        "    print()\n",
        "    print(f\"Agent Response: {message.content}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VEYcTShCsPaa"
      },
      "source": [
        "## Using Our Graph\n",
        "\n",
        "Now that we've created and compiled our graph - we can call it *just as we'd call any other* `Runnable`!\n",
        "\n",
        "Let's try out a few examples to see how it fairs:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qn4n37PQRPII",
        "outputId": "32ccce2a-9036-4cd1-b2a4-e7af986484b2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Initial Query: What is RAG in the context of Large Language Models? When did it break onto the scene?\n",
            "\n",
            "\n",
            "Agent Response: RAG, or Retrieval-Augmented Generation, is a technique used in the context of Large Language Models (LLMs) to improve their performance by combining retrieval-based methods with generative models. The core idea behind RAG is to enhance the generative capabilities of LLMs by incorporating relevant information retrieved from a large corpus of documents or a knowledge base. This approach helps in generating more accurate and contextually relevant responses, especially for tasks that require specific knowledge or up-to-date information.\n",
            "\n",
            "### Key Components of RAG:\n",
            "1. **Retriever**: This component is responsible for fetching relevant documents or passages from a large corpus based on the input query.\n",
            "2. **Generator**: This component generates the final response by conditioning on both the input query and the retrieved documents.\n",
            "\n",
            "### How RAG Works:\n",
            "1. **Query Input**: The user provides a query or prompt.\n",
            "2. **Retrieval Step**: The retriever searches a large corpus to find documents or passages that are relevant to the query.\n",
            "3. **Generation Step**: The generator takes the original query and the retrieved documents as input to generate a coherent and contextually appropriate response.\n",
            "\n",
            "### Advantages of RAG:\n",
            "- **Improved Accuracy**: By leveraging external knowledge, RAG can provide more accurate and detailed responses.\n",
            "- **Contextual Relevance**: The retrieval step ensures that the generated content is relevant to the specific context of the query.\n",
            "- **Scalability**: RAG can be scaled to work with very large corpora, making it suitable for a wide range of applications.\n",
            "\n",
            "### When Did RAG Break Onto the Scene?\n",
            "RAG was introduced in a research paper by Facebook AI (now Meta AI) in 2020. The paper, titled \"Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks,\" was presented at the Advances in Neural Information Processing Systems (NeurIPS) conference in 2020. The introduction of RAG marked a significant advancement in the field of natural language processing, particularly for tasks that require access to a large amount of external knowledge.\n",
            "\n",
            "Would you like more detailed information or references on this topic?\n"
          ]
        }
      ],
      "source": [
        "from langchain_core.messages import HumanMessage\n",
        "\n",
        "inputs = {\"messages\" : [HumanMessage(content=\"What is RAG in the context of Large Language Models? When did it break onto the scene?\")]}\n",
        "\n",
        "messages = app.invoke(inputs)\n",
        "\n",
        "print_messages(messages)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DBHnUtLSscRr"
      },
      "source": [
        "Let's look at what happened:\n",
        "\n",
        "1. Our state object was populated with our request\n",
        "2. The state object was passed into our entry point (agent node) and the agent node added an `AIMessage` to the state object and passed it along the conditional edge\n",
        "3. The conditional edge received the state object, found the \"function_call\" `additional_kwarg`, and sent the state object to the action node\n",
        "4. The action node added the response from the OpenAI function calling endpoint to the state object and passed it along the edge to the agent node\n",
        "5. The agent node added a response to the state object and passed it along the conditional edge\n",
        "6. The conditional edge received the state object, could not find the \"function_call\" `additional_kwarg` and passed the state object to END where we see it output in the cell above!\n",
        "\n",
        "Now let's look at an example that shows a multiple tool usage - all with the same flow!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "afv2BuEsV5JG",
        "outputId": "6ae3afec-7da7-4c5d-a3ca-2ed75735c029"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Initial Query: What is QLoRA in Machine Learning? Are their any technical papers that could help me understand? Once you have that information, can you look up the bio of the first author on the QLoRA paper?\n",
            "\n",
            "\n",
            "Tool Call - Name: arxiv + Query: {\"query\":\"QLoRA\"}\n",
            "Tool Response: Published: 2023-05-23\n",
            "Title: QLoRA: Efficient Finetuning of Quantized LLMs\n",
            "Authors: Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, Luke Zettlemoyer\n",
            "Summary: We present QLoRA, an efficient finetuning approach that reduces memory usage\n",
            "enough to finetune a 65B parameter model on a single 48GB GPU while preserving\n",
            "full 16-bit finetuning task performance. QLoRA backpropagates gradients through\n",
            "a frozen, 4-bit quantized pretrained language model into Low Rank\n",
            "Adapters~(LoRA). Our best model family, which we name Guanaco, outperforms all\n",
            "previous openly released models on the Vicuna benchmark, reaching 99.3% of the\n",
            "performance level of ChatGPT while only requiring 24 hours of finetuning on a\n",
            "single GPU. QLoRA introduces a number of innovations to save memory without\n",
            "sacrificing performance: (a) 4-bit NormalFloat (NF4), a new data type that is\n",
            "information theoretically optimal for normally distributed weights (b) double\n",
            "quantization to reduce the average memory footprint by quantizing the\n",
            "quantization constants, and (c) paged optimziers to manage memory spikes. We\n",
            "use QLoRA to finetune more than 1,000 models, providing a detailed analysis of\n",
            "instruction following and chatbot performance across 8 instruction datasets,\n",
            "multiple model types (LLaMA, T5), and model scales that would be infeasible to\n",
            "run with regular finetuning (e.g. 33B and 65B parameter models). Our results\n",
            "show that QLoRA finetuning on a small high-quality dataset leads to\n",
            "state-of-the-art results, even when using smaller models than the previous\n",
            "SoTA. We provide a detailed analysis of chatbot performance based on both human\n",
            "and GPT-4 evaluations showing that GPT-4 evaluations are a cheap and reasonable\n",
            "alternative to human evaluation. Furthermore, we find that current chatbot\n",
            "benchmarks are not trustworthy to accurately evaluate the performance levels of\n",
            "chatbots. A lemon-picked analysis demonstrates where Guanaco fails compared to\n",
            "ChatGPT. We release all of our models and code, including CUDA kernels for\n",
            "4-bit training.\n",
            "\n",
            "Published: 2024-05-27\n",
            "Title: Accurate LoRA-Finetuning Quantization of LLMs via Information Retention\n",
            "Authors: Haotong Qin, Xudong Ma, Xingyu Zheng, Xiaoyang Li, Yang Zhang, Shouda Liu, Jie Luo, Xianglong Liu, Michele Magno\n",
            "Summary: The LoRA-finetuning quantization of LLMs has been extensively studied to\n",
            "obtain accurate yet compact LLMs for deployment on resource-constrained\n",
            "hardware. However, existing methods cause the quantized LLM to severely degrade\n",
            "and even fail to benefit from the finetuning of LoRA. This paper proposes a\n",
            "novel IR-QLoRA for pushing quantized LLMs with LoRA to be highly accurate\n",
            "through information retention. The proposed IR-QLoRA mainly relies on two\n",
            "technologies derived from the perspective of unified information: (1)\n",
            "statistics-based Information Calibration Quantization allows the quantized\n",
            "parameters of LLM to retain original information accurately; (2)\n",
            "finetuning-based Information Elastic Connection makes LoRA utilizes elastic\n",
            "representation transformation with diverse information. Comprehensive\n",
            "experiments show that IR-QLoRA can significantly improve accuracy across LLaMA\n",
            "and LLaMA2 families under 2-4 bit-widths, e.g., 4- bit LLaMA-7B achieves 1.4%\n",
            "improvement on MMLU compared with the state-of-the-art methods. The significant\n",
            "performance gain requires only a tiny 0.31% additional time consumption,\n",
            "revealing the satisfactory efficiency of our IR-QLoRA. We highlight that\n",
            "IR-QLoRA enjoys excellent versatility, compatible with various frameworks\n",
            "(e.g., NormalFloat and Integer quantization) and brings general accuracy gains.\n",
            "The code is available at https://github.com/htqin/ir-qlora.\n",
            "\n",
            "Published: 2024-06-12\n",
            "Title: Exploring Fact Memorization and Style Imitation in LLMs Using QLoRA: An Experimental Study and Quality Assessment Methods\n",
            "Authors: Eugene Vyborov, Oleksiy Osypenko, Serge Sotnyk\n",
            "Summary: There are various methods for adapting LLMs to different domains. The most\n",
            "common methods are prompting, finetuning, and RAG. In this w\n",
            "\n",
            "Tool Call - Name: duckduckgo_search + Query: {\"query\":\"Tim Dettmers bio\"}\n",
            "Tool Response: Bio: Tim Dettmers's research focuses on making foundation models, such as ChatGPT, accessible to researchers and practitioners by reducing their resource requirements. This involves developing novel compression and networking algorithms and building systems that allow for memory-efficient, fast, and cheap deep learning. ... Bio: Tim Dettmers's research focuses on making foundation models, such as ChatGPT, accessible to researchers and practitioners by reducing their resource requirements. This involves developing novel compression and networking algorithms and building systems that allow for memory-efficient, fast, and cheap deep learning. ... Bio: Tim Dettmers's research focuses on making foundation models, such as ChatGPT, accessible to researchers and practitioners by reducing their resource requirements. This involves developing novel compression and networking algorithms and building systems that allow for memory-efficient, fast, and cheap deep learning. ... Bio: Tim Dettmers's research focuses on making foundation models, such as ChatGPT, accessible to researchers and practitioners by reducing their resource requirements. This involves developing novel compression and networking algorithms and building systems that allow for memory-efficient, fast, and cheap deep learning. ... Speaker Biography. Tim Dettmers' research focuses on making foundation models, such as ChatGPT, accessible to researchers and practitioners by reducing their resource requirements. This involves developing novel compression and networking algorithms and building systems that allow for memory-efficient, fast, and cheap deep learning.\n",
            "\n",
            "Agent Response: ### What is QLoRA in Machine Learning?\n",
            "\n",
            "QLoRA (Quantized Low Rank Adapters) is an efficient finetuning approach designed to reduce memory usage while preserving the performance of large language models (LLMs). The key features of QLoRA include:\n",
            "\n",
            "1. **Memory Efficiency**: It allows finetuning of a 65 billion parameter model on a single 48GB GPU.\n",
            "2. **Performance Preservation**: Maintains full 16-bit finetuning task performance.\n",
            "3. **Innovations**:\n",
            "   - **4-bit NormalFloat (NF4)**: A new data type optimal for normally distributed weights.\n",
            "   - **Double Quantization**: Reduces memory footprint by quantizing the quantization constants.\n",
            "   - **Paged Optimizers**: Manages memory spikes during training.\n",
            "\n",
            "The approach backpropagates gradients through a frozen, 4-bit quantized pretrained language model into Low Rank Adapters (LoRA). The best model family developed using QLoRA, named Guanaco, outperforms previous openly released models on the Vicuna benchmark, achieving 99.3% of ChatGPT's performance with only 24 hours of finetuning on a single GPU.\n",
            "\n",
            "### Technical Papers on QLoRA\n",
            "\n",
            "1. **Title**: [QLoRA: Efficient Finetuning of Quantized LLMs](https://arxiv.org/abs/2305.14314)\n",
            "   - **Authors**: Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, Luke Zettlemoyer\n",
            "   - **Summary**: This paper introduces QLoRA and details its innovations, performance, and applications. It also provides a detailed analysis of instruction following and chatbot performance across various datasets and model types.\n",
            "\n",
            "2. **Title**: [Accurate LoRA-Finetuning Quantization of LLMs via Information Retention](https://arxiv.org/abs/2305.14314)\n",
            "   - **Authors**: Haotong Qin, Xudong Ma, Xingyu Zheng, Xiaoyang Li, Yang Zhang, Shouda Liu, Jie Luo, Xianglong Liu, Michele Magno\n",
            "   - **Summary**: This paper proposes IR-QLoRA, which enhances the accuracy of quantized LLMs through information retention techniques.\n",
            "\n",
            "3. **Title**: [Exploring Fact Memorization and Style Imitation in LLMs Using QLoRA: An Experimental Study and Quality Assessment Methods](https://arxiv.org/abs/2305.14314)\n",
            "   - **Authors**: Eugene Vyborov, Oleksiy Osypenko, Serge Sotnyk\n",
            "   - **Summary**: This paper explores various methods for adapting LLMs to different domains using QLoRA.\n",
            "\n",
            "### Bio of Tim Dettmers\n",
            "\n",
            "Tim Dettmers is a researcher focused on making foundation models, such as ChatGPT, accessible to researchers and practitioners by reducing their resource requirements. His work involves developing novel compression and networking algorithms and building systems that allow for memory-efficient, fast, and cost-effective deep learning.\n"
          ]
        }
      ],
      "source": [
        "inputs = {\"messages\" : [HumanMessage(content=\"What is QLoRA in Machine Learning? Are their any technical papers that could help me understand? Once you have that information, can you look up the bio of the first author on the QLoRA paper?\")]}\n",
        "\n",
        "messages = app.invoke(inputs)\n",
        "\n",
        "print_messages(messages)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CXzDlZVz1Hnf"
      },
      "source": [
        "####üèóÔ∏è Activity #2:\n",
        "\n",
        "Please write out the steps the agent took to arrive at the correct answer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JQmrzYfrm1Dr"
      },
      "source": [
        "# ü§ù Breakout Room #2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pV3XeFOT1Sar"
      },
      "source": [
        "### Pre-processing for LangSmith"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wruQCuzewUuO"
      },
      "source": [
        "To do a little bit more preprocessing, let's wrap our LangGraph agent in a simple chain."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "oeXdQgbxwhTv"
      },
      "outputs": [],
      "source": [
        "def convert_inputs(input_object):\n",
        "  return {\"messages\" : [HumanMessage(content=input_object[\"question\"])]}\n",
        "\n",
        "def parse_output(input_state):\n",
        "  return input_state[\"messages\"][-1].content\n",
        "\n",
        "agent_chain = convert_inputs | app | parse_output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "id": "orYxBZXSxJjZ",
        "outputId": "d2b3be9d-1dbb-4109-83e4-673c9b73483e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"RAG stands for Retrieval-Augmented Generation. It is a technique used in natural language processing (NLP) that combines the strengths of retrieval-based and generation-based models to improve the quality and relevance of generated text.\\n\\nHere's a brief overview of how RAG works:\\n\\n1. **Retrieval**: The model first retrieves relevant documents or pieces of information from a large corpus based on the input query. This step ensures that the model has access to accurate and contextually relevant information.\\n\\n2. **Augmentation**: The retrieved documents are then used to augment the input query. This means that the information from the retrieved documents is combined with the original query to provide a richer context for the generation step.\\n\\n3. **Generation**: Finally, a generation model (often a transformer-based model like GPT) uses the augmented input to generate a response. The generation model can produce more accurate and contextually appropriate responses because it has access to the relevant information retrieved in the first step.\\n\\nRAG models are particularly useful in scenarios where the information required to generate a response is not contained within the model's parameters but can be found in external documents or databases. This approach helps in generating more informed and accurate responses, especially for complex queries that require specific knowledge.\""
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "agent_chain.invoke({\"question\" : \"What is RAG?\"})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f9UkCIqkpyZu"
      },
      "source": [
        "## Task 1: Creating An Evaluation Dataset\n",
        "\n",
        "Just as we saw last week, we'll want to create a dataset to test our Agent's ability to answer questions.\n",
        "\n",
        "In order to do this - we'll want to provide some questions and some answers. Let's look at how we can create such a dataset below.\n",
        "\n",
        "```python\n",
        "questions = [\n",
        "    \"What optimizer is used in QLoRA?\",\n",
        "    \"What data type was created in the QLoRA paper?\",\n",
        "    \"What is a Retrieval Augmented Generation system?\",\n",
        "    \"Who authored the QLoRA paper?\",\n",
        "    \"What is the most popular deep learning framework?\",\n",
        "    \"What significant improvements does the LoRA system make?\"\n",
        "]\n",
        "\n",
        "answers = [\n",
        "    {\"must_mention\" : [\"paged\", \"optimizer\"]},\n",
        "    {\"must_mention\" : [\"NF4\", \"NormalFloat\"]},\n",
        "    {\"must_mention\" : [\"ground\", \"context\"]},\n",
        "    {\"must_mention\" : [\"Tim\", \"Dettmers\"]},\n",
        "    {\"must_mention\" : [\"PyTorch\", \"TensorFlow\"]},\n",
        "    {\"must_mention\" : [\"reduce\", \"parameters\"]},\n",
        "]\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VfMXF2KAsQxs"
      },
      "source": [
        "####üèóÔ∏è Activity #3:\n",
        "\n",
        "Please create a dataset in the above format with at least 5 questions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "CbagRuJop83E"
      },
      "outputs": [],
      "source": [
        "questions = [\n",
        "    \"What optimizer is used in QLoRA?\",\n",
        "    \"What data type was created in the QLoRA paper?\",\n",
        "    \"What is a Retrieval Augmented Generation system?\",\n",
        "    \"Who authored the QLoRA paper?\",\n",
        "    \"What is the most popular deep learning framework?\",\n",
        "    \"What significant improvements does the LoRA system make?\"\n",
        "]\n",
        "\n",
        "answers = [\n",
        "    {\"must_mention\" : [\"paged\", \"optimizer\"]},\n",
        "    {\"must_mention\" : [\"NF4\", \"NormalFloat\"]},\n",
        "    {\"must_mention\" : [\"ground\", \"context\"]},\n",
        "    {\"must_mention\" : [\"Tim\", \"Dettmers\"]},\n",
        "    {\"must_mention\" : [\"PyTorch\", \"TensorFlow\"]},\n",
        "    {\"must_mention\" : [\"reduce\", \"parameters\"]},\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z7QVFuAmsh7L"
      },
      "source": [
        "Now we can add our dataset to our LangSmith project using the following code which we saw last Thursday!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "RLfrZrgSsn85"
      },
      "outputs": [],
      "source": [
        "from langsmith import Client\n",
        "\n",
        "client = Client()\n",
        "dataset_name = f\"Retrieval Augmented Generation - Evaluation Dataset - {uuid4().hex[0:8]}\"\n",
        "\n",
        "dataset = client.create_dataset(\n",
        "    dataset_name=dataset_name,\n",
        "    description=\"Questions about the QLoRA Paper to Evaluate RAG over the same paper.\"\n",
        ")\n",
        "\n",
        "client.create_examples(\n",
        "    inputs=[{\"question\" : q} for q in questions],\n",
        "    outputs=answers,\n",
        "    dataset_id=dataset.id,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ciV73F9Q04w0"
      },
      "source": [
        "#### ‚ùì Question #3:\n",
        "\n",
        "How are the correct answers associated with the questions?\n",
        "\n",
        "> NOTE: Feel free to indicate if this is problematic or not\n",
        "\n",
        "\n",
        "ANSWER: They are associated by element"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-lRTXUrTtP9Y"
      },
      "source": [
        "## Task 2: Adding Evaluators\n",
        "\n",
        "Now we can add a custom evaluator to see if our responses contain the expected information.\n",
        "\n",
        "We'll be using a fairly naive exact-match process to determine if our response contains specific strings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "QrAUXMFftlAY"
      },
      "outputs": [],
      "source": [
        "from langsmith.evaluation import EvaluationResult, run_evaluator\n",
        "\n",
        "@run_evaluator\n",
        "def must_mention(run, example) -> EvaluationResult:\n",
        "    prediction = run.outputs.get(\"output\") or \"\"\n",
        "    required = example.outputs.get(\"must_mention\") or []\n",
        "    score = all(phrase in prediction for phrase in required)\n",
        "    return EvaluationResult(key=\"must_mention\", score=score)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PNtHORUh0jZY"
      },
      "source": [
        "#### ‚ùì Question #4:\n",
        "\n",
        "What are some ways you could improve this metric as-is?\n",
        "\n",
        "> NOTE: Alternatively you can suggest where gaps exist in this method.\n",
        "\n",
        "\n",
        "ANSWER: We could use case sensitive approach and check spelling mistakes "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mZ4DVSXl0BX5"
      },
      "source": [
        "Now that we have created our custom evaluator - let's initialize our `RunEvalConfig` with it, and a few others:\n",
        "\n",
        "- `\"criteria\"` includes the default criteria which, in this case, means \"helpfulness\"\n",
        "- `\"cot_qa\"` includes a criteria that bases whether or not the answer is correct by utilizing a Chain of Thought prompt and the provided context to determine if the response is correct or not."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "sL4-XcjytWsu"
      },
      "outputs": [],
      "source": [
        "from langchain.smith import RunEvalConfig, run_on_dataset\n",
        "\n",
        "eval_config = RunEvalConfig(\n",
        "    custom_evaluators=[must_mention],\n",
        "    evaluators=[\n",
        "        \"criteria\",\n",
        "        \"cot_qa\",\n",
        "    ],\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r1RJr349zhv7"
      },
      "source": [
        "Task 3: Evaluating\n",
        "\n",
        "All that is left to do is evaluate our agent's response!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p5TeCUUkuGld",
        "outputId": "157f1019-4ee4-4994-fe80-7afc2e5fdfb6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "View the evaluation results for project 'RAG Pipeline - Evaluation - 382292a4' at:\n",
            "https://smith.langchain.com/o/fc1b336b-59a7-5844-8ab1-ad626257e5b9/datasets/27f09965-ee6d-4d1e-be5f-4524ab79184d/compare?selectedSessions=a212dc24-b492-468c-884f-0393e0efdb9e\n",
            "\n",
            "View all tests for Dataset Retrieval Augmented Generation - Evaluation Dataset - b3e4dea3 at:\n",
            "https://smith.langchain.com/o/fc1b336b-59a7-5844-8ab1-ad626257e5b9/datasets/27f09965-ee6d-4d1e-be5f-4524ab79184d\n",
            "[------------------------------------------------->] 6/6"
          ]
        },
        {
          "data": {
            "text/html": [
              "<h3>Experiment Results:</h3>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>feedback.helpfulness</th>\n",
              "      <th>feedback.COT Contextual Accuracy</th>\n",
              "      <th>feedback.must_mention</th>\n",
              "      <th>error</th>\n",
              "      <th>execution_time</th>\n",
              "      <th>run_id</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>6.0</td>\n",
              "      <td>6.000000</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>6.000000</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>unique</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>top</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>True</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>42fe2b3d-b15b-484c-8bdb-3b6fa86cf876</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>freq</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>5</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.833333</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>9.351210</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.408248</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.124977</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>7.887736</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>1.0</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>8.709497</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>1.0</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>9.454646</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>1.0</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>9.625329</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>1.0</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>11.166892</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        feedback.helpfulness  feedback.COT Contextual Accuracy  \\\n",
              "count                    6.0                          6.000000   \n",
              "unique                   NaN                               NaN   \n",
              "top                      NaN                               NaN   \n",
              "freq                     NaN                               NaN   \n",
              "mean                     1.0                          0.833333   \n",
              "std                      0.0                          0.408248   \n",
              "min                      1.0                          0.000000   \n",
              "25%                      1.0                          1.000000   \n",
              "50%                      1.0                          1.000000   \n",
              "75%                      1.0                          1.000000   \n",
              "max                      1.0                          1.000000   \n",
              "\n",
              "       feedback.must_mention error  execution_time  \\\n",
              "count                      6     0        6.000000   \n",
              "unique                     2     0             NaN   \n",
              "top                     True   NaN             NaN   \n",
              "freq                       5   NaN             NaN   \n",
              "mean                     NaN   NaN        9.351210   \n",
              "std                      NaN   NaN        1.124977   \n",
              "min                      NaN   NaN        7.887736   \n",
              "25%                      NaN   NaN        8.709497   \n",
              "50%                      NaN   NaN        9.454646   \n",
              "75%                      NaN   NaN        9.625329   \n",
              "max                      NaN   NaN       11.166892   \n",
              "\n",
              "                                      run_id  \n",
              "count                                      6  \n",
              "unique                                     6  \n",
              "top     42fe2b3d-b15b-484c-8bdb-3b6fa86cf876  \n",
              "freq                                       1  \n",
              "mean                                     NaN  \n",
              "std                                      NaN  \n",
              "min                                      NaN  \n",
              "25%                                      NaN  \n",
              "50%                                      NaN  \n",
              "75%                                      NaN  \n",
              "max                                      NaN  "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "{'project_name': 'RAG Pipeline - Evaluation - 382292a4',\n",
              " 'results': {'93f77a08-cd54-4d10-b1aa-0658c01e9bc5': {'input': {'question': 'What optimizer is used in QLoRA?'},\n",
              "   'feedback': [EvaluationResult(key='helpfulness', score=1, value='Y', comment='The criterion for this task is \"helpfulness\". \\n\\nThe submission provides a detailed explanation of the optimizer used in QLoRA, which is the \"paged optimizer\". It explains how this optimizer works and why it is used, which is to manage memory spikes and allow for efficient memory management. This information is directly related to the input question and provides a clear and comprehensive answer. \\n\\nThe submission also provides additional context about QLoRA, explaining that it is used for the fine-tuning of large language models and that it is part of a strategy to efficiently fine-tune quantized large language models. This information is not directly asked for in the input question, but it is relevant and provides additional insight into the topic. \\n\\nTherefore, the submission is helpful, insightful, and appropriate. \\n\\nY', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('9c964c7a-150e-4f1b-8a80-d43a2709b3a9'))}, feedback_config=None, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='COT Contextual Accuracy', score=1, value='CORRECT', comment=\"The student's answer correctly identifies that QLoRA uses paged optimizers. The student also provides additional context about why paged optimizers are used, which is not in conflict with the context provided. Therefore, the student's answer is correct.\\nGRADE: CORRECT\", correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('ac93e172-9761-4963-b73a-65db68b070d6'))}, feedback_config=None, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='must_mention', score=True, value=None, comment=None, correction=None, evaluator_info={}, feedback_config=None, source_run_id=UUID('9f84d909-2385-422a-b6fa-ad5281147d52'), target_run_id=None)],\n",
              "   'execution_time': 7.887736,\n",
              "   'run_id': '42fe2b3d-b15b-484c-8bdb-3b6fa86cf876',\n",
              "   'output': 'QLoRA (Quantized Low-Rank Adaptation) uses a combination of techniques to optimize memory usage and performance during the fine-tuning of large language models. One of the key innovations mentioned in the QLoRA paper is the use of \"paged optimizers\" to manage memory spikes. This approach allows for efficient memory management, enabling the fine-tuning of large models on hardware with limited memory capacity.\\n\\nIn summary, QLoRA employs paged optimizers as part of its strategy to efficiently fine-tune quantized large language models.',\n",
              "   'reference': {'must_mention': ['paged', 'optimizer']}},\n",
              "  '5f08c0bc-cb0e-444a-984d-3a25693e8c47': {'input': {'question': 'What data type was created in the QLoRA paper?'},\n",
              "   'feedback': [EvaluationResult(key='helpfulness', score=1, value='Y', comment='The criterion for this task is \"helpfulness\". \\n\\nThe submission provides a clear and direct answer to the question, stating that the QLoRA paper introduced a new data type called 4-bit NormalFloat (NF4). \\n\\nIn addition to providing the direct answer, the submission also provides additional information about the purpose and function of the 4-bit NormalFloat (NF4) data type. This additional information is relevant and could be helpful to someone seeking to understand the context or implications of the data type.\\n\\nThe submission is also appropriate, as it directly addresses the question and provides relevant, accurate information.\\n\\nTherefore, the submission meets the criterion of being helpful, insightful, and appropriate.\\n\\nY', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('d8ce1a88-b742-4d13-873f-c5c4cf6579e0'))}, feedback_config=None, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='COT Contextual Accuracy', score=1, value='CORRECT', comment=\"The student's answer matches the context given. The context mentions 'NF4' and 'NormalFloat', which the student correctly identifies as the new data type introduced in the QLoRA paper. The student also provides additional information about the purpose of this data type, which does not conflict with the context. Therefore, the student's answer is factually accurate.\\nGRADE: CORRECT\", correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('be8ed706-931e-46b7-b4ea-6834304da3c4'))}, feedback_config=None, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='must_mention', score=True, value=None, comment=None, correction=None, evaluator_info={}, feedback_config=None, source_run_id=UUID('34fc8324-2647-4a16-855f-b3897eddb419'), target_run_id=None)],\n",
              "   'execution_time': 9.50244,\n",
              "   'run_id': '69860a62-011a-4845-8e09-822b1420ce3c',\n",
              "   'output': 'In the QLoRA paper, a new data type called **4-bit NormalFloat (NF4)** was introduced. This data type is designed to be information-theoretically optimal for normally distributed weights, which helps in reducing memory usage while preserving performance during the finetuning of large language models.',\n",
              "   'reference': {'must_mention': ['NF4', 'NormalFloat']}},\n",
              "  'fb3f8ba0-9660-42c8-9046-5b1e58634eaa': {'input': {'question': 'What is a Retrieval Augmented Generation system?'},\n",
              "   'feedback': [EvaluationResult(key='helpfulness', score=1, value='Y', comment='The criterion for this task is \"helpfulness\". \\n\\n1. The submission provides a detailed explanation of what a Retrieval Augmented Generation system is, breaking down its components and explaining how they work together. This is helpful for someone trying to understand the concept.\\n\\n2. The submission also provides a step-by-step breakdown of how a RAG system works, which is insightful and helps to further clarify the concept.\\n\\n3. The submission goes beyond just explaining what a RAG system is and also discusses its benefits and applications. This additional information is appropriate and adds to the helpfulness of the submission.\\n\\n4. The language used in the submission is clear and easy to understand, which makes the information more accessible and therefore more helpful.\\n\\nBased on these points, the submission meets the criterion of being helpful, insightful, and appropriate.\\n\\nY', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('d5a04183-06d9-4942-8829-a15cad2d6c3f'))}, feedback_config=None, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='COT Contextual Accuracy', score=1, value='CORRECT', comment=\"The student's answer is detailed and comprehensive. It accurately describes what a Retrieval-Augmented Generation (RAG) system is, how it works, its benefits, and its applications. The student explains the two main components of a RAG system, the retrieval-based component and the generation-based component, and how they work together to improve the quality and relevance of generated text. The student also provides examples of techniques and models used in these components. The student then outlines the process of how a RAG system works, from receiving an input query to generating a response. The student also discusses the benefits of a RAG system, such as improved relevance, knowledge integration, and contextual understanding. Finally, the student mentions some applications of RAG systems, including question answering, customer support, and content creation. Therefore, the student's answer is factually accurate and complete.\\nGRADE: CORRECT\", correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('23bfb9e4-5d65-47b3-84c7-2578ff7bb8df'))}, feedback_config=None, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='must_mention', score=False, value=None, comment=None, correction=None, evaluator_info={}, feedback_config=None, source_run_id=UUID('af3fa305-a230-4047-a3e1-43bec446a64d'), target_run_id=None)],\n",
              "   'execution_time': 11.166892,\n",
              "   'run_id': '89f83884-2f56-4b85-891d-540545e32721',\n",
              "   'output': 'Retrieval-Augmented Generation (RAG) is a type of natural language processing (NLP) system that combines retrieval-based and generation-based approaches to improve the quality and relevance of generated text. Here‚Äôs a breakdown of the two components:\\n\\n1. **Retrieval-Based Component**:\\n   - This part of the system retrieves relevant documents or pieces of information from a large corpus or database based on the input query.\\n   - It uses techniques such as TF-IDF, BM25, or more advanced neural retrieval models to find the most relevant documents.\\n\\n2. **Generation-Based Component**:\\n   - This part of the system generates text based on the retrieved documents and the input query.\\n   - It typically uses advanced language models like GPT-3, BERT, or other transformer-based models to generate coherent and contextually appropriate responses.\\n\\n### How RAG Works:\\n1. **Input Query**: The system receives an input query from the user.\\n2. **Retrieval**: The retrieval component searches a large corpus to find documents or passages that are relevant to the query.\\n3. **Augmentation**: The retrieved documents are then used to augment the input query, providing additional context and information.\\n4. **Generation**: The generation component uses the augmented input to generate a response that is informed by the retrieved documents.\\n\\n### Benefits of RAG:\\n- **Improved Relevance**: By leveraging external documents, the system can provide more accurate and relevant responses.\\n- **Knowledge Integration**: It can integrate up-to-date information from a large corpus, making it more knowledgeable.\\n- **Contextual Understanding**: The generation component can produce more contextually appropriate and coherent responses.\\n\\n### Applications:\\n- **Question Answering**: Providing detailed and accurate answers to user queries.\\n- **Customer Support**: Assisting in customer service by retrieving and generating relevant responses.\\n- **Content Creation**: Aiding in the creation of articles, reports, and other content by providing relevant information and context.\\n\\nOverall, RAG systems aim to combine the strengths of both retrieval and generation to create more powerful and effective NLP applications.',\n",
              "   'reference': {'must_mention': ['ground', 'context']}},\n",
              "  'e66504f8-1b89-4e47-902d-780880ae49b8': {'input': {'question': 'Who authored the QLoRA paper?'},\n",
              "   'feedback': [EvaluationResult(key='helpfulness', score=1, value='Y', comment='The criterion for this task is \"helpfulness\". \\n\\nThe submission provides a clear and direct answer to the question, stating the authors of the QLoRA paper. It also provides additional information about the title of the paper and its publication date, which could be helpful for someone looking for more context or trying to locate the paper. \\n\\nThe information is presented in a clear and organized manner, making it easy to understand. \\n\\nThe submission is appropriate as it directly addresses the question and provides relevant information. \\n\\nBased on these observations, the submission meets the criterion of being helpful. \\n\\nY', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('bb43fd15-7f96-4510-9f95-8a8ce4d18130'))}, feedback_config=None, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='COT Contextual Accuracy', score=0, value='INCORRECT', comment=\"The context only mentions 'Tim' and 'Dettmers' as the authors of the QLoRA paper. The student's answer includes these two names, but also adds 'Artidoro Pagnoni', 'Ari Holtzman', and 'Luke Zettlemoyer'. Since the context does not mention these additional names, the student's answer contains conflicting information. Therefore, the student's answer is incorrect.\\nGRADE: INCORRECT\", correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('0d5e364a-29eb-4a10-a757-e8de492da9fe'))}, feedback_config=None, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='must_mention', score=True, value=None, comment=None, correction=None, evaluator_info={}, feedback_config=None, source_run_id=UUID('cd8c311b-a5be-4c73-b5b6-367e2a2b22f7'), target_run_id=None)],\n",
              "   'execution_time': 9.406852,\n",
              "   'run_id': '23301a4f-4a90-434b-9848-1b40ca04b51c',\n",
              "   'output': 'The QLoRA paper titled \"QLoRA: Efficient Finetuning of Quantized LLMs\" was authored by:\\n\\n- Tim Dettmers\\n- Artidoro Pagnoni\\n- Ari Holtzman\\n- Luke Zettlemoyer\\n\\nThe paper was published on May 23, 2023.',\n",
              "   'reference': {'must_mention': ['Tim', 'Dettmers']}},\n",
              "  '76cff750-d9e7-4351-9d61-8f15a5693559': {'input': {'question': 'What is the most popular deep learning framework?'},\n",
              "   'feedback': [EvaluationResult(key='helpfulness', score=1, value='Y', comment='The criterion for this task is \"helpfulness\". \\n\\nThe submission provides a detailed answer to the question, listing the three most popular deep learning frameworks as of 2023. For each framework, the submission provides a brief description, including who developed it, its main features, and why it is popular. This information is relevant and useful to someone wanting to understand the landscape of deep learning frameworks.\\n\\nThe submission is also insightful, as it not only lists the frameworks but also explains why they are popular. This gives the reader a deeper understanding of the factors that contribute to a framework\\'s popularity.\\n\\nThe submission is appropriate as it directly answers the question and provides relevant information. It does not include any inappropriate or off-topic content.\\n\\nBased on this analysis, the submission meets the criterion of being helpful, insightful, and appropriate.\\n\\nY', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('109d13e7-a34c-4c6a-9474-43499a5a013e'))}, feedback_config=None, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='COT Contextual Accuracy', score=1, value='CORRECT', comment=\"The student's answer mentions both TensorFlow and PyTorch, which are the two deep learning frameworks provided in the context. The student also provides additional information about each framework, including their developers and why they are popular. The student also mentions Keras, which is not mentioned in the context, but this does not contradict the information in the context. Therefore, the student's answer is factually accurate based on the context provided.\\nGRADE: CORRECT\", correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('dd89b7e5-653e-4e77-a71d-806034270369'))}, feedback_config=None, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='must_mention', score=True, value=None, comment=None, correction=None, evaluator_info={}, feedback_config=None, source_run_id=UUID('b9c5c857-8b4b-49eb-816a-e7d0dc325a2c'), target_run_id=None)],\n",
              "   'execution_time': 9.666292,\n",
              "   'run_id': '501368e7-8254-402f-bcf9-8c15b289cf26',\n",
              "   'output': \"As of 2023, the most popular deep learning frameworks are:\\n\\n1. **TensorFlow**: Developed by Google, TensorFlow is one of the most widely used open-source libraries for numerical computation and deep learning. It has a strong community and is used by major corporations like Airbnb and Intel.\\n\\n2. **PyTorch**: Developed by Facebook's AI Research lab, PyTorch is highly favored in the research community for its ease of use, GPU capabilities, and excellent debugging tools. It is often the framework of choice for state-of-the-art models.\\n\\n3. **Keras**: Initially developed by Fran√ßois Chollet, Keras is a high-level neural network API written in Python. It runs on top of TensorFlow, Theano, and CNTK, and is known for its simplicity and ease of use.\\n\\nThese frameworks are popular due to their robust features, active community support, and extensive use in both industry and research.\",\n",
              "   'reference': {'must_mention': ['PyTorch', 'TensorFlow']}},\n",
              "  '4c209c23-c27e-42ec-86a1-2cab08137fbf': {'input': {'question': 'What significant improvements does the LoRA system make?'},\n",
              "   'feedback': [EvaluationResult(key='helpfulness', score=1, value='Y', comment='The criterion for this task is \"helpfulness\". To determine if the submission meets this criterion, we need to assess if the information provided is helpful, insightful, and appropriate.\\n\\n1. **Helpfulness**: The submission provides a detailed explanation of the improvements made by the LoRA system. It lists seven significant improvements, each with a clear explanation. This would be very helpful to someone trying to understand the benefits of the LoRA system.\\n\\n2. **Insightfulness**: The submission goes beyond just listing the improvements. It explains why these improvements are significant and how they impact the field of machine learning. This shows a deep understanding of the topic and provides valuable insights.\\n\\n3. **Appropriateness**: The submission directly answers the question asked in the input. It stays on topic and provides relevant information. The language used is professional and suitable for an academic or technical context.\\n\\nBased on this analysis, the submission meets the criterion of helpfulness. It provides a helpful, insightful, and appropriate response to the input question.\\n\\nY', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('ae6f764d-fe5f-4f18-af33-28d06d25b2a5'))}, feedback_config=None, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='COT Contextual Accuracy', score=1, value='CORRECT', comment=\"The context provided is 'reduce' and 'parameters'. The student's answer mentions that the LoRA system significantly reduces the number of trainable parameters required for fine-tuning large models. This directly addresses the context provided. The student's answer also provides additional information about the benefits of the LoRA system, none of which contradict the context or the question. Therefore, the student's answer is factually accurate.\\nGRADE: CORRECT\", correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('4810b275-4a29-4283-94da-0cf8b0d9b336'))}, feedback_config=None, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='must_mention', score=True, value=None, comment=None, correction=None, evaluator_info={}, feedback_config=None, source_run_id=UUID('2e901e9b-0b28-4d77-9e33-3b674b21cd3b'), target_run_id=None)],\n",
              "   'execution_time': 8.477046,\n",
              "   'run_id': '9f90514c-2f0e-4d21-82e3-0e65e7784942',\n",
              "   'output': 'LoRA, or Low-Rank Adaptation, is a technique used in the field of machine learning, particularly in the context of fine-tuning large language models. Here are some significant improvements that the LoRA system makes:\\n\\n1. **Parameter Efficiency**: LoRA significantly reduces the number of trainable parameters required for fine-tuning large models. Instead of updating all the parameters of the model, LoRA introduces low-rank matrices that adapt the pre-trained weights, making the process more efficient.\\n\\n2. **Memory Efficiency**: By reducing the number of parameters that need to be updated, LoRA also reduces the memory footprint during training. This is particularly beneficial when working with very large models that would otherwise require substantial computational resources.\\n\\n3. **Faster Training**: With fewer parameters to update, the training process becomes faster. This allows for quicker iterations and experimentation, which is valuable in research and development settings.\\n\\n4. **Cost-Effectiveness**: The reduction in computational resources and time translates to lower costs for training models. This makes it more feasible for organizations with limited budgets to fine-tune large models.\\n\\n5. **Scalability**: LoRA makes it easier to scale the fine-tuning process to very large models, which would be impractical to fine-tune using traditional methods due to resource constraints.\\n\\n6. **Flexibility**: The low-rank adaptation approach can be applied to various parts of the model, providing flexibility in how the fine-tuning is performed. This allows for targeted adaptations that can improve performance on specific tasks.\\n\\n7. **Performance**: Despite the reduction in the number of trainable parameters, models fine-tuned with LoRA often achieve comparable or even superior performance to those fine-tuned with traditional methods. This is because LoRA effectively captures the essential adaptations needed for the target task.\\n\\nOverall, LoRA provides a more efficient, cost-effective, and scalable approach to fine-tuning large language models, making it a valuable technique in the field of machine learning.',\n",
              "   'reference': {'must_mention': ['reduce', 'parameters']}}},\n",
              " 'aggregate_metrics': None}"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "client.run_on_dataset(\n",
        "    dataset_name=dataset_name,\n",
        "    llm_or_chain_factory=agent_chain,\n",
        "    evaluation=eval_config,\n",
        "    verbose=True,\n",
        "    project_name=f\"RAG Pipeline - Evaluation - {uuid4().hex[0:8]}\",\n",
        "    project_metadata={\"version\": \"1.0.0\"},\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w1wKRddbIY_S"
      },
      "source": [
        "## Part 2:\n",
        "\n",
        "Now that we've done evaluation - let's see if we can add an extra step where we review the content we've generated to confirm if it fully answers the user's query!\n",
        "\n",
        "We're going to make a few key adjustments to account for this:\n",
        "\n",
        "1. We're going to add an artificial limit on how many \"loops\" the agent can go through - this will help us to avoid the potential situation where we never exit the loop.\n",
        "2. We'll add a custom node and conditional edge to determine if the response was helpful enough."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "npTYJ8ayR5B3"
      },
      "source": [
        "First, let's define our state again - we can check the length of the state object, so we don't need additional state for this."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "-LQ84YhyJG0w"
      },
      "outputs": [],
      "source": [
        "class AgentState(TypedDict):\n",
        "  messages: Annotated[list, add_messages]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gC8t-4FISCEh"
      },
      "source": [
        "We're going to add a custom helpfulness check here!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "ZV_PxI5zNY7f"
      },
      "outputs": [],
      "source": [
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "def check_helpfulness(state):\n",
        "  initial_query = state[\"messages\"][0]\n",
        "  final_response = state[\"messages\"][-1]\n",
        "\n",
        "  if len(state[\"messages\"]) > 10:\n",
        "    return \"END\"\n",
        "\n",
        "  prompt_template = \"\"\"\\\n",
        "  Given an initial query and a final response, determine if the final response is extremely helpful or not. Please indicate helpfulness with a 'Y' and unhelpfulness as an 'N'.\n",
        "\n",
        "  Initial Query:\n",
        "  {initial_query}\n",
        "\n",
        "  Final Response:\n",
        "  {final_response}\"\"\"\n",
        "\n",
        "  prompt_template = PromptTemplate.from_template(prompt_template)\n",
        "\n",
        "  helpfulness_check_model = ChatOpenAI(model=\"gpt-4\")\n",
        "\n",
        "  helpfulness_chain = prompt_template | helpfulness_check_model | StrOutputParser()\n",
        "\n",
        "  helpfulness_response = helpfulness_chain.invoke({\"initial_query\" : initial_query.content, \"final_response\" : final_response.content})\n",
        "\n",
        "  if \"Y\" in helpfulness_response:\n",
        "    print(\"Helpful!\")\n",
        "    return \"end\"\n",
        "  else:\n",
        "    print(\"Not helpful!\")\n",
        "    return \"continue\"\n",
        "\n",
        "def dummy_node(state):\n",
        "  return"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fz1u9Vf4SHxJ"
      },
      "source": [
        "####üèóÔ∏è Activity #4:\n",
        "\n",
        "Please write what is happening in our `check_helpfulness` function!\n",
        "\n",
        "ANSWER:\n",
        "\n",
        "Extracts the initial and final messages from a state dictionary.\n",
        "Checks if there are more than 10 messages and returns \"END\" if true.\n",
        "Defines a prompt template to assess the helpfulness of the final response.\n",
        "Creates a helpfulness check model using GPT-4.\n",
        "Chains the prompt template, model, and output parser.\n",
        "Invokes the chain to determine if the final response is helpful.\n",
        "Prints and returns based on the helpfulness check.\n",
        "Returns \"continue\" by default if no conditions are met.\n",
        "Includes a dummy function as a placeholder."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sD7EV0HqSQcb"
      },
      "source": [
        "Now we can set our graph up! This process will be almost entirely the same - with the inclusion of one additional node/conditional edge!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oajBwLkFVi1N"
      },
      "source": [
        "####üèóÔ∏è Activity #5:\n",
        "\n",
        "Please write markdown for the following cells to explain what each is doing."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M6rN7feNVn9f"
      },
      "source": [
        "Setting the StateGraph and adding nodes to that object"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "6r6XXA5FJbVf"
      },
      "outputs": [],
      "source": [
        "graph_with_helpfulness_check = StateGraph(AgentState)\n",
        "\n",
        "graph_with_helpfulness_check.add_node(\"agent\", call_model)\n",
        "graph_with_helpfulness_check.add_node(\"action\", call_tool)\n",
        "graph_with_helpfulness_check.add_node(\"passthrough\", dummy_node)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XZ22o2mWVrfp"
      },
      "source": [
        "Setting the entrypoint"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "HNWHwWxuRiLY"
      },
      "outputs": [],
      "source": [
        "graph_with_helpfulness_check.set_entry_point(\"agent\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6BhnBW2YVsJO"
      },
      "source": [
        "Adding conditional edges"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "aVTKnWMbP_8T"
      },
      "outputs": [],
      "source": [
        "graph_with_helpfulness_check.add_conditional_edges(\n",
        "    \"agent\",\n",
        "    should_continue,\n",
        "    {\n",
        "        \"continue\" : \"action\",\n",
        "        \"end\" : \"passthrough\"\n",
        "    }\n",
        ")\n",
        "\n",
        "graph_with_helpfulness_check.add_conditional_edges(\n",
        "    \"passthrough\",\n",
        "    check_helpfulness,\n",
        "    {\n",
        "        \"continue\" : \"agent\",\n",
        "        \"end\" : END\n",
        "    }\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZGDLEWOIVtK0"
      },
      "source": [
        "Add connection between action and agent node"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "cbDK2MbuREgU"
      },
      "outputs": [],
      "source": [
        "graph_with_helpfulness_check.add_edge(\"action\", \"agent\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rSI8AOaEVvT-"
      },
      "source": [
        "Compile the agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "oQldl8ERQ8lf"
      },
      "outputs": [],
      "source": [
        "agent_with_helpfulness_check = graph_with_helpfulness_check.compile()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F67FGCMRVwGz"
      },
      "source": [
        "Test the retriever"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B3oo8E-PRK1T",
        "outputId": "6bc30282-41af-489c-d67d-e457c55821e0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Helpful!\n",
            "Initial Query: Related to machine learning, what is LoRA? Also, who is Tim Dettmers? Also, what is Attention?\n",
            "\n",
            "\n",
            "Agent Response: Let's break down each of these terms and individuals:\n",
            "\n",
            "### LoRA (Low-Rank Adaptation)\n",
            "LoRA, or Low-Rank Adaptation, is a technique used in machine learning, particularly in the context of fine-tuning large pre-trained models. The idea behind LoRA is to adapt a pre-trained model to a new task by introducing a low-rank decomposition of the weight updates. This allows for efficient fine-tuning with fewer parameters, which can be particularly useful when dealing with large models that are computationally expensive to train.\n",
            "\n",
            "### Tim Dettmers\n",
            "Tim Dettmers is a researcher known for his work in the field of machine learning, particularly in the areas of efficient training and inference of large neural networks. He has contributed to the development of techniques that make it feasible to train large models on consumer-grade hardware. His research often focuses on optimizing the computational aspects of machine learning to make advanced models more accessible and efficient.\n",
            "\n",
            "### Attention\n",
            "Attention is a mechanism used in neural networks, particularly in the context of sequence-to-sequence models like those used in natural language processing (NLP). The attention mechanism allows the model to focus on different parts of the input sequence when generating each part of the output sequence. This is particularly useful in tasks like machine translation, where the model needs to align words in the source language with words in the target language.\n",
            "\n",
            "The most well-known form of attention is the \"self-attention\" mechanism used in Transformer models. In self-attention, each word in the input sequence is compared with every other word to compute a set of attention weights. These weights determine how much focus the model should place on each word when generating the output. This allows the model to capture long-range dependencies and relationships between words, which is crucial for understanding context and meaning in language.\n",
            "\n",
            "Would you like more detailed information on any of these topics?\n"
          ]
        }
      ],
      "source": [
        "inputs = {\"messages\" : [HumanMessage(content=\"Related to machine learning, what is LoRA? Also, who is Tim Dettmers? Also, what is Attention?\")]}\n",
        "\n",
        "messages = agent_with_helpfulness_check.invoke(inputs)\n",
        "\n",
        "print_messages(messages)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
